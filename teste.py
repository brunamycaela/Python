# -*- coding: utf-8 -*-
"""Teste.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jLvVSuvUC33vkc7GV7gwX_kqbqCAre7c
"""

# libs necessarias
import pandas as pd
import numpy as np

# libs graficas
import matplotlib.pyplot as plt
import seaborn as sns

#Avisos
import warnings
warnings.filterwarnings('ignore')

# configuração o maximo de linhas e colunas na horar de mostra a informação.
pd.set_option('display.max_rows', 200) 
pd.set_option('display.max_columns', 100)

#configuração no Matplotlib
plt.rcParams['figure.figsize'] = (15, 6) # tamanho do graficos
plt.style.use('seaborn-darkgrid') # estilo do grafico para dark

"""# **Exploração dos dados**"""

# Lendo os dados
Base_Dados = pd.read_csv('house_data.csv')

# Dimensão
Base_Dados.shape

# verificando
Base_Dados.head()

# Quanto vale o aluguel da sua casa?
# vamos treinar o modelo para saber respoder a pergunta...

# Removendo colunas 
Base_Dados.drop(columns=['fire insurance (R$)','total (R$)'], inplace=True)

# verificar 
Base_Dados.shape # inf numero de linhas e colunas
#(linhas, Colunas)

# Campos vazios
Base_Dados.isnull().sum().sort_values( ascending=False )
# sum vai somar
# sort_values vai ordenar do maior para o menor

# Campos unicos
Base_Dados.nunique()
# verificar quantos campos unicos tem na base

# tipos de colunas
Base_Dados.dtypes

# Tipos de colunas
Base_Dados.info()

"""# **Exploração Analitica (EDA)**"""

# Filtrar os tipos de colunas
Colunas_Categoricas = Base_Dados.columns[ Base_Dados.dtypes == object ]
Colunas_Numericas = Base_Dados.columns[ Base_Dados.dtypes != object ]

Colunas_Categoricas, Colunas_Numericas
# filtramos por somente int e por somente object

# Analise dos campos objetos
Base_Dados['city'].value_counts( normalize=True ) * 100
# value_counts para contar quantos registros em cada cidade
# normalize= True)* 100 tras a porcentagem

# fazer a naliser usando for 

# loop
for Coluna in Colunas_Categoricas:

  # Fazendo a analise
  analise = Base_Dados[Coluna].value_counts(normalize=True) * 100

  # mostrando a analise em tela
  print(Coluna,'\n',analise, '\n')

# correção dos dados 

# ajusta o Andar
Base_Dados.loc[Base_Dados['floor'] == '301'] # loc aqui vai localizar
# depois de localizar utilizar o indice para o proximo passo
Base_Dados.iloc[ 2562,5] = 30
# iloc localiza o indice aonde fica a linhas e depois a coluna especifica e alterar para 30

# ajustar o '-'
Base_Dados['floor'] = Base_Dados['floor'].apply( lambda Registro : 0 if Registro == '-' else Registro )
# na coluna floor tiver um '-' então ele me retorna no lugar um 0

# transformar o tipo da coluna floor para inteiro
Base_Dados['floor'] = pd.to_numeric( Base_Dados['floor'] )

# Verificar
Base_Dados.head()

# Grid - Gráficos

# Tamanho
Figura, Eixo = plt.subplots( figsize=(20, 30) )

# Cor de fundo
Cor_Fundo = '#f5f5f5'
Figura.set_facecolor( Cor_Fundo )

# Paleta de Cores
Paleta_Cores = sns.color_palette( 'flare', len(Colunas_Numericas) * 2 )

# Titulo
plt.suptitle('Análise das Variaveis Numericas', fontsize=22, color='#404040', fontweight=600 )

# Estrutura
Linhas = 7 # (Todas as infos numericas)
Colunas = 2 #( Boxplot - Distplot)
Posicao = 1 # Posicao inicial do grid

# Loop para plotar os gráficos
for Coluna in Colunas_Numericas:

  # Plot no Grid -- Boxplot
  plt.subplot( Linhas, Colunas, Posicao )

  # Titulo
  plt.title( f'{Coluna}', loc='left', fontsize=14, fontweight=200 )

  # Plot
  sns.boxplot( data=Base_Dados, y=Coluna, showmeans=True, saturation=0.75, 
              linewidth=1, color=Paleta_Cores[Posicao], width=0.25 )

  # Mudar
  Posicao += 1

  # Plot no Grid -- Distplot
  plt.subplot( Linhas, Colunas, Posicao )

  # Titulo
  plt.title( f'{Coluna}', loc='left', fontsize=14, fontweight=200 )

  # Plot
  sns.distplot( Base_Dados[Coluna], color=Paleta_Cores[ Posicao - 1 ] )

  # Mudar
  Posicao += 1

# Ajute de Grid
plt.subplots_adjust( top=0.95, hspace=0.3 )

Base_Dados.loc[ Base_Dados['area'] <= 10000]['area'].describe()

"""# **Eng de Features**"""

# ajusta das colunas categorias animal
# com a função map podemos converter os dados de acep para 1 ...
Base_Dados['animal'] = Base_Dados['animal'].map({'acept':1, 'not acept':0})

# ajustar a coluna furniture	para realizar converção.
Base_Dados['furniture'] = Base_Dados['furniture'].map({'furnished':1, 'not furnished':0})

# filtrar a cidade de são paulo
Filtro_SP = Base_Dados.loc[Base_Dados['city'] == 'São Paulo']

# Verificar
Filtro_SP.head()

# retirando a Coluna Cidade
Filtro_SP.drop( columns=['city'], inplace=True)

# Separa os dados
Caracteristicas = Filtro_SP.drop( columns=['rent amount (R$)'])
Previsor = Filtro_SP['rent amount (R$)']

# verificar
Caracteristicas.shape, Previsor.shape

# variavel caracteristicas
Caracteristicas.head()

# variavel Previsor
Previsor.head()

# correlação
Filtro_SP.corr()

# proxima de 1 - corelação positiva [Ambas sobem]
# proxima de -1 - corelação negativa [uma sobe outra desce]

# Biblioteca Yellowbrick 
from yellowbrick.features import Rank2D

# Definir o metodo 
Correlacao = Rank2D( algoritmo='pearson')

# Fitar Função
Correlacao.fit(Caracteristicas, Previsor)
Correlacao.transform(Caracteristicas)
Correlacao.show();

# Separa os dados 
from sklearn.model_selection import train_test_split

# Divisão dos Dados
x_treino, x_teste, y_treino, y_teste = train_test_split(
    Caracteristicas, Previsor, test_size=0.2, random_state=10
)

print(f'Dados de Treino: {x_treino.shape[0]}')
print(f'Dados de teste: {x_teste.shape[0]}')

# features mais relevantes
from sklearn.feature_selection import mutual_info_regression
from sklearn.feature_selection import SelectKBest

# seleção de features
def Selecao_Features(x_treino, y_treino):

  # configurar para selecionar as features
  Selecao = SelectKBest(score_func=mutual_info_regression, k='all')

  # Fitar o aprendizado
  Selecao.fit(x_treino, y_treino)

  return Selecao


# Aplicar essa função 
Scores = Selecao_Features(x_treino,y_treino)

# Analisar
for Posicao, Score in enumerate(Scores.scores_):
  print(f'{x_treino.columns[Posicao]}: {Score}')

"""# **Construção do Modelo**"""

# Modelo Random forest Regresson
from sklearn.ensemble import RandomForestRegressor

# Instanciar 
Modelo_Floresta = RandomForestRegressor(max_depth=5)

# Fitar
Modelo_Floresta.fit(x_treino, y_treino)

# treinamos o modelo floresta

# Avaliar a performance
Previsoes = Modelo_Floresta.predict(x_teste)

# Função para avaliar
from sklearn.metrics import mean_absolute_error, r2_score
from math import sqrt


# Avaliando o modelo 
print(f'RMSE: { sqrt(mean_absolute_error(y_teste, Previsoes) ) }')
print(f'Score: { r2_score(y_teste, Previsoes)}')

# avaliando com a biblioteca Yellowbrink
from yellowbrick.regressor import PredictionError

#I Instanciar
Modelo = RandomForestRegressor(max_depth=5)
Erro_Modelo = PredictionError(Modelo)

# fitar
Erro_Modelo.fit(x_treino, y_treino)
Erro_Modelo.score(x_teste, y_teste)
Erro_Modelo.show();

"""# **Exportação**"""

# Exportando o Modelo
import joblib

# Função
joblib.dump(Modelo_Floresta, 'Modelo_froresta_aleatorio_v100.pkl')

# Testando modelo treinado

Funcao_modelo_carredado = joblib.load('Modelo_froresta_aleatorio_v100.pkl')

# Testando
Funcao_modelo_carredado.predict(x_teste.head(1).values)

x_teste.head(1).values